# AI-Powered Knowledge Base Search & Enrichment

A lightweight **Retrieval-Augmented Generation (RAG)** system that lets users upload documents, search them in natural language, get AI-generated answers, and receive smart suggestions when information is incomplete.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gradio UI  â”‚ â”€â–¶  â”‚ FastAPI APIâ”‚ â”€â–¶  â”‚ Qdrant DB  â”‚ â”€â–¶  â”‚ LLM Engine â”‚
â”‚ (Frontend) â”‚     â”‚ (Backend)  â”‚     â”‚ (Vectors)  â”‚     â”‚ (Answer +  â”‚
â”‚            â”‚     â”‚            â”‚     â”‚            â”‚     â”‚  Critic)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* **Frontend:** Gradio (3 tabs â€” File, Collection, Chat)
* **Backend:** FastAPI REST API
* **Vector DB:** Qdrant (semantic search)
* **LLM Layer:** Answer generator + Critic evaluator

---

## Functionality

| Feature                | Description                                         |
| ---------------------- | --------------------------------------------------- |
| **Upload Documents**   | Add PDFs or text files via UI or API                |
| **Create Collections** | Group related files for search                      |
| **Link Content**       | Process files into embeddings (BGE-large-en-v1.5)   |
| **Ask Questions**      | Natural language query across documents             |
| **Reranker**           | Improves relevance of retrieved chunks              |
| **Critic Head**        | Evaluates if the answer is complete or uncertain    |
| **Auto-Enrichment**    | Suggests topics or documents to fill knowledge gaps |

---

## How It Works

1. **Upload & Index** â€” Files are embedded using **BGE-large-en-v1.5** and stored in **Qdrant**.
2. **Query** â€” A user asks a natural-language question.
3. **Retrieve** â€” Top-matching chunks are fetched via vector similarity.
4. **Rerank** â€” Results are refined using a **CrossEncoder**.
5. **Generate** â€” The **LLM** produces an answer using retrieved context.
6. **Critic Evaluate** â€” A critic model checks for missing information and suggests enrichment areas.

---

## Example Output

```json
{
  "answer": "LearnLM is part of Gemini 2.5 designed to infuse AI with learning science principles.",
  "confidence": 0.82,
  "is_relevant": true,
  "critic": {
    "confidence": 0.6,
    "missing_info": "Lacks examples of LearnLM in Google products.",
    "enrichment_suggestions": [
      "Add details about LearnLM applications in Search and Classroom.",
      "Include its underlying learning science principles."
    ]
  }
}
```

## Design Decisions and Tradeoffs

1. **Modular Architecture** â€” The system is split into independent layers (FastAPI, Qdrant, Reranker, Critic) to ensure clean separation of concerns and easy scalability.
2. **Qdrant Selection** â€” Chosen over Pinecone and ChromaDB for its open-source flexibility, production-grade performance, and seamless local + cloud deployment options.
3. **Reranker Integration** â€” A lightweight cross-encoder (MS MARCO MiniLM) was chosen for better precision without adding GPU dependency.
4. **Critic Head** â€” Implemented as a secondary reasoning model that validates and enriches answers post-generation.
5. **Feedback Learning System** â€” Added user-based rating mechanism (ğŸ‘/ğŸ‘) to simulate RLHF-style reinforcement on document retrieval quality.
6. **Structured Output Schema** â€” JSON schema ensures consistency for future integrations and automated evaluations.
7. **Gradio UI** â€” Selected over traditional web frameworks to rapidly prototype a polished frontend in limited time.

---

## Limitations

| Limitation                        | Reason                                       |
| --------------------------------- | -------------------------------------------- |
| **High Query Latency (~20â€“30s)**  | Sequential model loading and CPU inference   |
| **In-memory Qdrant on HF Spaces** | No persistence between restarts              |
| **Limited Auto-Enrichment**       | Placeholder logic (suggestive, not fetching) |
| **Single-threaded LLM calls**     | Slower for long or multi-query workflows     |

---

## Next Steps

1. **Optimize Latency** â€” Move to async or batched inference with GPU backend.
2. **Persistent Vector DB** â€” Use managed Qdrant or Pinecone cloud instance.
3. **Expand Enrichment Engine** â€” Integrate Wikipedia / ArXiv fetcher for missing info.
4. **Add Feedback Loop** â€” Allow user rating to fine-tune critic.
5. **Auto-Enrichment** â€” Fetch missing data from trusted external sources for validation.
6. **User Feedback Integration** â€” Allow users to rate answer quality and use this signal to improve model reliability.
7. **Deploy Production-Grade Stack** â€” Split frontend/backend, add auth & caching.

---

## Summary

âœ… **End-to-End RAG Pipeline**
âœ… **Structured AI Responses** (Answer + Confidence + Critic)
âœ… **Smart Completeness Detection**
âœ… **Knowledge Enrichment Suggestions**

---
